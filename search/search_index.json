{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Azure Datalake Utils \u00b6 Utilidades para interactuar con Azure Datalake Documentation: https://centraal_api.github.io/azure_datalake_utils GitHub: https://github.com/centraal_api/azure_datalake_utils PyPI: https://pypi.org/project/azure_datalake_utils/ Free software: Apache-2.0 Features \u00b6 TODO Credits \u00b6 This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Home"},{"location":"#azure-datalake-utils","text":"Utilidades para interactuar con Azure Datalake Documentation: https://centraal_api.github.io/azure_datalake_utils GitHub: https://github.com/centraal_api/azure_datalake_utils PyPI: https://pypi.org/project/azure_datalake_utils/ Free software: Apache-2.0","title":"Azure Datalake Utils"},{"location":"#features","text":"TODO","title":"Features"},{"location":"#credits","text":"This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Credits"},{"location":"api/","text":"Interactua con el datalake. Modulos exportados por este paquete. Datalake : Clase principal para interactuar con el datalake. Main module. Datalake \u00b6 Bases: object Clase para representar operaciones de Datalake. Source code in azure_datalake_utils/azure_datalake_utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class Datalake ( object ): \"\"\"Clase para representar operaciones de Datalake.\"\"\" def __init__ ( self , datalake_name : str , tenant_id : str ) -> None : \"\"\"Clase para interactuar con Azure Dalake. Args: datalake_name: nombre de la cuenta de Azure Datalake Gen2. tenant_id: Identificador del tenant, es valor es proporcionado por arquitectura de datos, debe conservarse para un correcto funcionamiento. \"\"\" self . datalake_name = datalake_name self . tenant_id = tenant_id credentials = InteractiveBrowserCredential ( tenant_id = self . tenant_id ) credentials . authenticate () self . _credentials = credentials self . storage_options = { 'account_name' : self . datalake_name , 'anon' : False } def read_csv ( self , ruta : str , ** kwargs ) -> pd . DataFrame : \"\"\"Leer un archivo CSV desde la cuenta de datalake. # noqa: E501 Esta funci\u00f3n hace una envoltura de [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.csv` o `.txt`. Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv`. **kwargs: argumentos a pasar a pd.read_csv. Returns: Dataframe con la informacion del la ruta. \"\"\" try : df = pd . read_csv ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) except IndexError : raise ArchivoNoEncontrado ( ruta ) return df __init__ ( datalake_name , tenant_id ) \u00b6 Clase para interactuar con Azure Dalake. Parameters: Name Type Description Default datalake_name str nombre de la cuenta de Azure Datalake Gen2. required tenant_id str Identificador del tenant, es valor es proporcionado por arquitectura de datos, debe conservarse para un correcto funcionamiento. required Source code in azure_datalake_utils/azure_datalake_utils.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , datalake_name : str , tenant_id : str ) -> None : \"\"\"Clase para interactuar con Azure Dalake. Args: datalake_name: nombre de la cuenta de Azure Datalake Gen2. tenant_id: Identificador del tenant, es valor es proporcionado por arquitectura de datos, debe conservarse para un correcto funcionamiento. \"\"\" self . datalake_name = datalake_name self . tenant_id = tenant_id credentials = InteractiveBrowserCredential ( tenant_id = self . tenant_id ) credentials . authenticate () self . _credentials = credentials self . storage_options = { 'account_name' : self . datalake_name , 'anon' : False } read_csv ( ruta , ** kwargs ) \u00b6 Leer un archivo CSV desde la cuenta de datalake. noqa: E501 \u00b6 Esta funci\u00f3n hace una envoltura de pandas.read_csv . Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. Parameters: Name Type Description Default ruta str Ruta a leeder el archivo, debe contener una referencia a un archivo .csv o .txt . Recordar que la ruta debe contener esta estructura: {NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv . required **kwargs argumentos a pasar a pd.read_csv. {} Returns: Type Description pd . DataFrame Dataframe con la informacion del la ruta. Source code in azure_datalake_utils/azure_datalake_utils.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def read_csv ( self , ruta : str , ** kwargs ) -> pd . DataFrame : \"\"\"Leer un archivo CSV desde la cuenta de datalake. # noqa: E501 Esta funci\u00f3n hace una envoltura de [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.csv` o `.txt`. Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv`. **kwargs: argumentos a pasar a pd.read_csv. Returns: Dataframe con la informacion del la ruta. \"\"\" try : df = pd . read_csv ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) except IndexError : raise ArchivoNoEncontrado ( ruta ) return df","title":"Modules"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake","text":"Bases: object Clase para representar operaciones de Datalake. Source code in azure_datalake_utils/azure_datalake_utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class Datalake ( object ): \"\"\"Clase para representar operaciones de Datalake.\"\"\" def __init__ ( self , datalake_name : str , tenant_id : str ) -> None : \"\"\"Clase para interactuar con Azure Dalake. Args: datalake_name: nombre de la cuenta de Azure Datalake Gen2. tenant_id: Identificador del tenant, es valor es proporcionado por arquitectura de datos, debe conservarse para un correcto funcionamiento. \"\"\" self . datalake_name = datalake_name self . tenant_id = tenant_id credentials = InteractiveBrowserCredential ( tenant_id = self . tenant_id ) credentials . authenticate () self . _credentials = credentials self . storage_options = { 'account_name' : self . datalake_name , 'anon' : False } def read_csv ( self , ruta : str , ** kwargs ) -> pd . DataFrame : \"\"\"Leer un archivo CSV desde la cuenta de datalake. # noqa: E501 Esta funci\u00f3n hace una envoltura de [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.csv` o `.txt`. Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv`. **kwargs: argumentos a pasar a pd.read_csv. Returns: Dataframe con la informacion del la ruta. \"\"\" try : df = pd . read_csv ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) except IndexError : raise ArchivoNoEncontrado ( ruta ) return df","title":"Datalake"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake.__init__","text":"Clase para interactuar con Azure Dalake. Parameters: Name Type Description Default datalake_name str nombre de la cuenta de Azure Datalake Gen2. required tenant_id str Identificador del tenant, es valor es proporcionado por arquitectura de datos, debe conservarse para un correcto funcionamiento. required Source code in azure_datalake_utils/azure_datalake_utils.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , datalake_name : str , tenant_id : str ) -> None : \"\"\"Clase para interactuar con Azure Dalake. Args: datalake_name: nombre de la cuenta de Azure Datalake Gen2. tenant_id: Identificador del tenant, es valor es proporcionado por arquitectura de datos, debe conservarse para un correcto funcionamiento. \"\"\" self . datalake_name = datalake_name self . tenant_id = tenant_id credentials = InteractiveBrowserCredential ( tenant_id = self . tenant_id ) credentials . authenticate () self . _credentials = credentials self . storage_options = { 'account_name' : self . datalake_name , 'anon' : False }","title":"__init__()"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake.read_csv","text":"Leer un archivo CSV desde la cuenta de datalake.","title":"read_csv()"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake.read_csv--noqa-e501","text":"Esta funci\u00f3n hace una envoltura de pandas.read_csv . Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. Parameters: Name Type Description Default ruta str Ruta a leeder el archivo, debe contener una referencia a un archivo .csv o .txt . Recordar que la ruta debe contener esta estructura: {NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv . required **kwargs argumentos a pasar a pd.read_csv. {} Returns: Type Description pd . DataFrame Dataframe con la informacion del la ruta. Source code in azure_datalake_utils/azure_datalake_utils.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def read_csv ( self , ruta : str , ** kwargs ) -> pd . DataFrame : \"\"\"Leer un archivo CSV desde la cuenta de datalake. # noqa: E501 Esta funci\u00f3n hace una envoltura de [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.csv` o `.txt`. Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv`. **kwargs: argumentos a pasar a pd.read_csv. Returns: Dataframe con la informacion del la ruta. \"\"\" try : df = pd . read_csv ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) except IndexError : raise ArchivoNoEncontrado ( ruta ) return df","title":"noqa: E501"},{"location":"changelog/","text":"Changelog \u00b6 0.1.1 - 2022-08-30 \u00b6 Added \u00b6 inlcuir primera utilidad 0.0.1 (2022-08-30) \u00b6 First release on PyPI.","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#011---2022-08-30","text":"","title":"0.1.1 - 2022-08-30"},{"location":"changelog/#added","text":"inlcuir primera utilidad","title":"Added"},{"location":"changelog/#001-2022-08-30","text":"First release on PyPI.","title":"0.0.1 (2022-08-30)"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/centraal_api/azure-datalake-utils/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 azure_datalake_utils could always use more documentation, whether as part of the official azure_datalake_utils docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/centraal_api/azure_datalake_utils/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up azure_datalake_utils for local development. Fork the azure_datalake_utils repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/azure_datalake_utils.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.8 and 3.9. Check https://github.com/centraal_api/azure-datalake-utils/actions and make sure that the tests pass for all supported Python versions. Tips \u00b6 $ poetry run pytest tests/test_azure_datalake_utils.py To run a subset of tests. Deploying \u00b6 A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/centraal_api/azure-datalake-utils/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"azure_datalake_utils could always use more documentation, whether as part of the official azure_datalake_utils docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/centraal_api/azure_datalake_utils/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up azure_datalake_utils for local development. Fork the azure_datalake_utils repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/azure_datalake_utils.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.8 and 3.9. Check https://github.com/centraal_api/azure-datalake-utils/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#tips","text":"$ poetry run pytest tests/test_azure_datalake_utils.py To run a subset of tests.","title":"Tips"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install azure_datalake_utils, run this command in your terminal: $ pip install azure_datalake_utils This is the preferred method to install azure_datalake_utils, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for azure_datalake_utils can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/centraal_api/azure-datalake-utils Or download the tarball : $ curl -OJL https://github.com/centraal_api/azure-datalake-utils/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install azure_datalake_utils, run this command in your terminal: $ pip install azure_datalake_utils This is the preferred method to install azure_datalake_utils, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for azure_datalake_utils can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/centraal_api/azure-datalake-utils Or download the tarball : $ curl -OJL https://github.com/centraal_api/azure-datalake-utils/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"From source"},{"location":"usage/","text":"Usage \u00b6 To use azure_datalake_utils in a project import azure_datalake_utils","title":"Usage"},{"location":"usage/#usage","text":"To use azure_datalake_utils in a project import azure_datalake_utils","title":"Usage"}]}