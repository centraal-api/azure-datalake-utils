{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Azure Datalake Utils \u00b6 Utilidades para interactuar con Azure Datalake. El objetivo es evitar que personas denominadas cientificos ciudadanos tengan que interactuar con librerias, que no son, totalmente relacionadas con el analisis de datos. La hipotesis detras de este pensamiento es que se puede lograr incrementar la adopci\u00f3n de estas herramientas si se facilitan y simplifica la interacci\u00f3n de pandas con la lectura del datalake. Documentation: https://centraal-api.github.io/azure-datalake-utils GitHub: https://github.com/centraal-api/azure-datalake-utils PyPI: https://pypi.org/project/azure-datalake-utils/ Free software: Apache-2.0 Features \u00b6 Control de autenticaci\u00f3n directamente con el Directorio activo de Azure. Lectura de archivos csv, excel, json y parquet de una forma m\u00e1s concisa. Creaci\u00f3n de token SAS para generar URL a un path especifico. Publicar nueva version \u00b6 Seguir checklist del template orginal . Credits \u00b6 La librer\u00eda es creada y mantenida por Centraal Studio . Centraal Studio Agredece la alianza con Haceb , cuyos retos internos de democratizar el acceso a informaci\u00f3n han motivado la creaci\u00f3n de esta librer\u00eda. // This package is created and mantained by Centraal Studio . Centraal Studio appreciate the alliance with Haceb , which internal efforts to democratize the access of company information has motivated the creation of the library. This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Home"},{"location":"#azure-datalake-utils","text":"Utilidades para interactuar con Azure Datalake. El objetivo es evitar que personas denominadas cientificos ciudadanos tengan que interactuar con librerias, que no son, totalmente relacionadas con el analisis de datos. La hipotesis detras de este pensamiento es que se puede lograr incrementar la adopci\u00f3n de estas herramientas si se facilitan y simplifica la interacci\u00f3n de pandas con la lectura del datalake. Documentation: https://centraal-api.github.io/azure-datalake-utils GitHub: https://github.com/centraal-api/azure-datalake-utils PyPI: https://pypi.org/project/azure-datalake-utils/ Free software: Apache-2.0","title":"Azure Datalake Utils"},{"location":"#features","text":"Control de autenticaci\u00f3n directamente con el Directorio activo de Azure. Lectura de archivos csv, excel, json y parquet de una forma m\u00e1s concisa. Creaci\u00f3n de token SAS para generar URL a un path especifico.","title":"Features"},{"location":"#publicar-nueva-version","text":"Seguir checklist del template orginal .","title":"Publicar nueva version"},{"location":"#credits","text":"La librer\u00eda es creada y mantenida por Centraal Studio . Centraal Studio Agredece la alianza con Haceb , cuyos retos internos de democratizar el acceso a informaci\u00f3n han motivado la creaci\u00f3n de esta librer\u00eda. // This package is created and mantained by Centraal Studio . Centraal Studio appreciate the alliance with Haceb , which internal efforts to democratize the access of company information has motivated the creation of the library. This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Credits"},{"location":"api/","text":"Interactua con el datalake. Modulos exportados por este paquete. Datalake : Clase principal para interactuar con el datalake. Main module. Datalake \u00b6 Bases: object Clase para representar operaciones de Datalake. Source code in azure_datalake_utils/azure_datalake_utils.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 class Datalake ( object ): \"\"\"Clase para representar operaciones de Datalake.\"\"\" def __init__ ( self , datalake_name : str , tenant_id : str , account_key : Optional [ str ] = None , fsspec_cache : bool = True ) -> None : \"\"\"Clase para interactuar con Azure Dalake. Args: datalake_name: nombre de la cuenta de Azure Datalake Gen2. tenant_id: Identificador del tenant, es valor es proporcionado por arquitectura de datos, debe conservarse para un correcto funcionamiento. account_key: key de la cuenta. Por defecto es None y es ignorado. fsspec_cache: indica si se va usar la funcionalidad de cache de fsspec. **NOTA**: se recomienda usar `fsspec_cache=False` cuando se encuentran en ambientes serverless, donde no se tiene control escricto de la `vida` de la instancias, de esta manera la libreria siempre verifica la informaci\u00f3n con Azure y no reusar instancias en cache que pueden tener valores desactualizados. Para ver los efectos, ver los siguientes issues: - https://github.com/fsspec/adlfs/issues/391 - https://github.com/Azure/azure-sdk-for-python/issues/28312 \"\"\" self . datalake_name = datalake_name if account_key is None : self . tenant_id = tenant_id credential = exp . AioCredentialWrapper ( InteractiveBrowserCredential ( tenant_id = self . tenant_id )) # TODO: verificar https://github.com/fsspec/adlfs/issues/270 # para ver como evoluciona y evitar este condicional. if platform . system () . lower () != 'windows' : storage_options = { 'account_name' : self . datalake_name , 'anon' : False } else : storage_options = { 'account_name' : self . datalake_name , 'anon' : False , 'credential' : credential , } else : storage_options = { 'account_name' : self . datalake_name , 'account_key' : account_key } self . fs = AzureBlobFileSystem ( account_name = self . datalake_name , account_key = account_key ) if not fsspec_cache : storage_options [ \"default_cache_type\" ] = None storage_options [ \"default_fill_cache\" ] = False storage_options [ \"skip_instance_cache\" ] = True self . storage_options = storage_options @classmethod def from_account_key ( cls , datalake_name : str , account_key : str , fsspec_cache : bool = True ): \"\"\"Opcion de inicializar con account key.\"\"\" return cls ( datalake_name = datalake_name , account_key = account_key , tenant_id = None , fsspec_cache = fsspec_cache ) @raiseArchivoNoEncontrado def read_csv ( self , ruta : Union [ str , List [ str ]], ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo CSV desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_csv]. usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [pd.read_csv]: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.csv` o `.txt`. Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv`. NUEVO en version 0.5: Tambien acepta una lista de archivos terminados en csv. ejemplo: ``` [{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv, {NOMBRE_CONTENEDOR}/{RUTA2}/{nombre o patron}.csv] ``` **kwargs: argumentos a pasar a pd.read_csv. El unico argumento que es ignorado es storage_options. Returns: Dataframe con la informacion del la ruta. \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) if type ( ruta ) == str : self . _verificar_extension ( ruta , '.csv' , '.txt' , '.tsv' ) df = pd . read_csv ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) else : [ self . _verificar_extension ( r , '.csv' , '.txt' , '.tsv' ) for r in ruta ] rutas = [ pd . read_csv ( f \"az:// { r } \" , storage_options = self . storage_options , ** kwargs ) for r in ruta ] df = pd . concat ( rutas , ignore_index = True ) return df @raiseArchivoNoEncontrado def read_excel ( self , ruta : str , experimental : bool = False , ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo Excel desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_excel]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_excel]]:(https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.xlsx` o `.xls`. Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.xlsx`. force_client: Bandera para forzar el uso del cliente. Es flag todavia es experimental Y en futuras versiones se va ha eliminar. Solo funciona con un solo archivo de excel. **kwargs: argumentos a pasar a pd.read_excel. Returns: Dataframe con la informacion del la ruta. \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) if 'engine' in kwargs : kwargs . pop ( 'engine' ) self . _verificar_extension ( ruta , '.xlsx' , '.xls' ) # TODO: esto es algo temporal y se debe analizar si se puede remover. Esta bandera fue necesario debido a: # 1. Si no se usa el cliente para descargar el excel cuando se modifica el archivo de # excel y no se ha reiniciado # el runtime se provoca el error: `BadZipFile(\"File is not a zip file\")`. # 2. Se debe analizar como integrar en windows donde se usa credenciales asociadas al Active directory. if experimental : df = exp . read_excel_with_client ( ruta , self . datalake_name , self . storage_options [ 'account_key' ], ** kwargs ) return df ############### df = pd . read_excel ( f \"az:// { ruta } \" , engine = 'openpyxl' , storage_options = self . storage_options , ** kwargs ) return df @raiseArchivoNoEncontrado def read_json ( self , ruta : str , ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo Json desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_json]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_json]]:(https://pandas.pydata.org/docs/reference/api/pandas.read_json.html) Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.json` . Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.json`. **kwargs: argumentos a pasar a pd.read_json. Returns: Dataframe con la informacion del la ruta. \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) self . _verificar_extension ( ruta , '.json' ) df = pd . read_json ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) return df @raiseArchivoNoEncontrado def read_parquet ( self , ruta : str , ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo parquet desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_parquet]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_parquet]]:(https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.read_parquet.html) Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.json` . Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.json`. **kwargs: argumentos a pasar a pd.read_parquet. Returns: dataframe con la ruta \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) df = pd . read_parquet ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) return df def read_csv_with_partition ( self , ruta : str , partition_cols : Dict [ str , List [ str ]] = None , partition_exclusion : Dict [ str , List [ str ]] = None , partition_inclusion : Dict [ str , List [ str ]] = None , last_modified_last_level : bool = True , ** kwargs : Optional [ Any ], ) -> pd . DataFrame : \"\"\"Leer un archivo CSV desde la cuenta de datalake con particiones Hive. Una partici\u00f3n tipo Hive son archivos almacenados de la forma /ruta/to/archivo/particion_1=1/particion2=2/archivo_con_info.extension. **IMPORTANTE**: por el momento el funcionamiento de esta funci\u00f3n es solo soportada si el objeto fue creado mediante `from_account_key`. Esta funci\u00f3n hace una envoltura de `read_csv` que asu vez usa [pd.read_csv]. usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [pd.read_csv]: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html Args: ruta: Ruta a leeder el archivo, esta ruta debe contener archivos guardados siguiendo la convecci\u00f3n de la partici\u00f3n Hive. Ejemplo si la estructura de los archivos es: - contenedor/ruta/al/archivo/year=2022/month=10/load_date=2022-01-01/archivo.csv - contenedor/ruta/al/archivo/year=2022/month=10/load_date=2022-01-02/archivo.csv - contenedor/ruta/al/archivo/year=2022/month=11/load_date=2022-01-01/archivo.csv `ruta` debe ser `contenedor/ruta/al/archivo/`. partition_cols: Definir que particionews incluir, se asume que se sabe a priori la estructura. Si se pasa `None`, se activa el descubrimiento automatico de las particiones. partition_exclusion: Definir que particiones excluir, es m\u00e1s util cuando no se ha definido `partition_cols=None`. partition_inclusion: Definir la particiones a incluir, es m\u00e1s util cuando no se ha definido `partition_cols=None`. last_modified_last_level: Define si el ultimo nivel no se tiene en cuenta para particiones y se carga el archivo que fue modificado de manera m\u00e1s reciente. **kwargs: argumentos a pasar a pd.read_csv. El unico argumento que es ignorado es storage_options. Returns: Dataframe con la informacion del la ruta. \"\"\" if not ruta . endswith ( \"/\" ): raise ValueError ( \"ruta debe finalizar en /\" ) particiones = HivePartitiion ( ruta = ruta , partition_cols = partition_cols , partition_exclusion = partition_exclusion , partition_inclusion = partition_inclusion , last_modified_last_level = last_modified_last_level , fs = self . fs , ) list_of_files = particiones . get_partition_list () list_of_dfs = [] for path_ , particion in zip ( list_of_files , particiones . get_partition_files ()): particiones_cols = particion [ 1 ] df = self . read_csv ( path_ , ** kwargs ) . assign ( ** particiones_cols ) list_of_dfs . append ( df ) return pd . concat ( list_of_dfs , ignore_index = True ) def write_csv ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo.\"\"\" if not self . _verificar_extension ( ruta , '.csv' , '.txt' , '.tsv' ): raise ExtensionIncorrecta ( ruta ) sep = kwargs . get ( 'sep' , ',' ) df_to_write = self . _limpiar_df_cols_str ( df , sep ) df_to_write . to_csv ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) def write_excel ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo al datalake.\"\"\" if not self . _verificar_extension ( ruta , '.xlsx' , '.xls' ): raise ExtensionIncorrecta ( ruta ) df . to_excel ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) def write_json ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo al datalake.\"\"\" if not self . _verificar_extension ( ruta , '.json' ): raise ExtensionIncorrecta ( ruta ) df . to_json ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) def write_parquet ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo al datalake.\"\"\" df . to_parquet ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) def generar_url_con_sas_token ( self , path : str , duration : int , unit : Literal [ \"day\" , \"hour\" , \"minute\" , 'second' ] = \"hour\" , ip : str = None ) -> str : \"\"\"Genera un link sas.\"\"\" return create_url_sas_token ( path , self . fs , duration , unit , ip ) def _verificar_extension ( self , ruta : str , * extensiones ): \"\"\"Metodo para verificar extensiones.\"\"\" for ext in extensiones : verificar = ruta . endswith ( ext ) if verificar : return True raise ExtensionIncorrecta ( ruta ) def _es_carpeta ( self , ruta : str ): \"\"\"Metodo para verificar si es una carpeta.\"\"\" if ruta . endswith ( \"/\" ): return True raise ExtensionIncorrecta ( f \" { ruta } No termina en /\" ) def _limpiar_df_cols_str ( self , df : pd . DataFrame , sep : str = \",\" ) -> pd . DataFrame : \"\"\"Limpia las columnas string del dataframe.\"\"\" types = df . dtypes string_columns = list ( types [ types == np . array ([ object ()]) . dtype ] . index ) esc_sep = re . escape ( sep ) df_res = df . copy () df_res [ string_columns ] = ( df [ string_columns ] . replace ( esc_sep , \" \" , regex = True ) . replace ( \" \\r \" , \" \" , regex = True ) . replace ( \" \\n \" , \" \" , regex = True ) ) return df_res __init__ ( datalake_name , tenant_id , account_key = None , fsspec_cache = True ) \u00b6 Clase para interactuar con Azure Dalake. Parameters: Name Type Description Default datalake_name str nombre de la cuenta de Azure Datalake Gen2. required tenant_id str Identificador del tenant, es valor es proporcionado por arquitectura de datos, debe conservarse para un correcto funcionamiento. required account_key Optional [ str ] key de la cuenta. Por defecto es None y es ignorado. None fsspec_cache bool indica si se va usar la funcionalidad de cache de fsspec. NOTA : se recomienda usar fsspec_cache=False cuando se encuentran en ambientes serverless, donde no se tiene control escricto de la vida de la instancias, de esta manera la libreria siempre verifica la informaci\u00f3n con Azure y no reusar instancias en cache que pueden tener valores desactualizados. Para ver los efectos, ver los siguientes issues: - https://github.com/fsspec/adlfs/issues/391 - https://github.com/Azure/azure-sdk-for-python/issues/28312 True Source code in azure_datalake_utils/azure_datalake_utils.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , datalake_name : str , tenant_id : str , account_key : Optional [ str ] = None , fsspec_cache : bool = True ) -> None : \"\"\"Clase para interactuar con Azure Dalake. Args: datalake_name: nombre de la cuenta de Azure Datalake Gen2. tenant_id: Identificador del tenant, es valor es proporcionado por arquitectura de datos, debe conservarse para un correcto funcionamiento. account_key: key de la cuenta. Por defecto es None y es ignorado. fsspec_cache: indica si se va usar la funcionalidad de cache de fsspec. **NOTA**: se recomienda usar `fsspec_cache=False` cuando se encuentran en ambientes serverless, donde no se tiene control escricto de la `vida` de la instancias, de esta manera la libreria siempre verifica la informaci\u00f3n con Azure y no reusar instancias en cache que pueden tener valores desactualizados. Para ver los efectos, ver los siguientes issues: - https://github.com/fsspec/adlfs/issues/391 - https://github.com/Azure/azure-sdk-for-python/issues/28312 \"\"\" self . datalake_name = datalake_name if account_key is None : self . tenant_id = tenant_id credential = exp . AioCredentialWrapper ( InteractiveBrowserCredential ( tenant_id = self . tenant_id )) # TODO: verificar https://github.com/fsspec/adlfs/issues/270 # para ver como evoluciona y evitar este condicional. if platform . system () . lower () != 'windows' : storage_options = { 'account_name' : self . datalake_name , 'anon' : False } else : storage_options = { 'account_name' : self . datalake_name , 'anon' : False , 'credential' : credential , } else : storage_options = { 'account_name' : self . datalake_name , 'account_key' : account_key } self . fs = AzureBlobFileSystem ( account_name = self . datalake_name , account_key = account_key ) if not fsspec_cache : storage_options [ \"default_cache_type\" ] = None storage_options [ \"default_fill_cache\" ] = False storage_options [ \"skip_instance_cache\" ] = True self . storage_options = storage_options from_account_key ( datalake_name , account_key , fsspec_cache = True ) classmethod \u00b6 Opcion de inicializar con account key. Source code in azure_datalake_utils/azure_datalake_utils.py 70 71 72 73 @classmethod def from_account_key ( cls , datalake_name : str , account_key : str , fsspec_cache : bool = True ): \"\"\"Opcion de inicializar con account key.\"\"\" return cls ( datalake_name = datalake_name , account_key = account_key , tenant_id = None , fsspec_cache = fsspec_cache ) generar_url_con_sas_token ( path , duration , unit = 'hour' , ip = None ) \u00b6 Genera un link sas. Source code in azure_datalake_utils/azure_datalake_utils.py 303 304 305 306 307 def generar_url_con_sas_token ( self , path : str , duration : int , unit : Literal [ \"day\" , \"hour\" , \"minute\" , 'second' ] = \"hour\" , ip : str = None ) -> str : \"\"\"Genera un link sas.\"\"\" return create_url_sas_token ( path , self . fs , duration , unit , ip ) read_csv ( ruta , ** kwargs ) \u00b6 Leer un archivo CSV desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de pd.read_csv . usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. Parameters: Name Type Description Default ruta Union [ str , List [ str ]] Ruta a leeder el archivo, debe contener una referencia a un archivo .csv o .txt . Recordar que la ruta debe contener esta estructura: {NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv . NUEVO en version 0.5: Tambien acepta una lista de archivos terminados en csv. ejemplo: [{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv, {NOMBRE_CONTENEDOR}/{RUTA2}/{nombre o patron}.csv] required **kwargs Optional [ Any ] argumentos a pasar a pd.read_csv. El unico argumento que es ignorado es storage_options. {} Returns: Type Description pd . DataFrame Dataframe con la informacion del la ruta. Source code in azure_datalake_utils/azure_datalake_utils.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @raiseArchivoNoEncontrado def read_csv ( self , ruta : Union [ str , List [ str ]], ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo CSV desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_csv]. usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [pd.read_csv]: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.csv` o `.txt`. Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv`. NUEVO en version 0.5: Tambien acepta una lista de archivos terminados en csv. ejemplo: ``` [{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv, {NOMBRE_CONTENEDOR}/{RUTA2}/{nombre o patron}.csv] ``` **kwargs: argumentos a pasar a pd.read_csv. El unico argumento que es ignorado es storage_options. Returns: Dataframe con la informacion del la ruta. \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) if type ( ruta ) == str : self . _verificar_extension ( ruta , '.csv' , '.txt' , '.tsv' ) df = pd . read_csv ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) else : [ self . _verificar_extension ( r , '.csv' , '.txt' , '.tsv' ) for r in ruta ] rutas = [ pd . read_csv ( f \"az:// { r } \" , storage_options = self . storage_options , ** kwargs ) for r in ruta ] df = pd . concat ( rutas , ignore_index = True ) return df read_csv_with_partition ( ruta , partition_cols = None , partition_exclusion = None , partition_inclusion = None , last_modified_last_level = True , ** kwargs ) \u00b6 Leer un archivo CSV desde la cuenta de datalake con particiones Hive. Una partici\u00f3n tipo Hive son archivos almacenados de la forma /ruta/to/archivo/particion_1=1/particion2=2/archivo_con_info.extension. IMPORTANTE : por el momento el funcionamiento de esta funci\u00f3n es solo soportada si el objeto fue creado mediante from_account_key . Esta funci\u00f3n hace una envoltura de read_csv que asu vez usa pd.read_csv . usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. Parameters: Name Type Description Default ruta str Ruta a leeder el archivo, esta ruta debe contener archivos guardados siguiendo required la convecci\u00f3n de la partici\u00f3n Hive. Ejemplo si la estructura de los archivos es required partition_cols Dict [ str , List [ str ]] Definir que particionews incluir, se asume que se sabe a priori la estructura. Si se pasa None , se activa el descubrimiento automatico de las particiones. None partition_exclusion Dict [ str , List [ str ]] Definir que particiones excluir, es m\u00e1s util cuando no se ha definido partition_cols=None . None partition_inclusion Dict [ str , List [ str ]] Definir la particiones a incluir, es m\u00e1s util cuando no se ha definido partition_cols=None . None last_modified_last_level bool Define si el ultimo nivel no se tiene en cuenta para particiones y se carga el archivo que fue modificado de manera m\u00e1s reciente. True **kwargs Optional [ Any ] argumentos a pasar a pd.read_csv. El unico argumento que es ignorado es storage_options. {} Returns: Type Description pd . DataFrame Dataframe con la informacion del la ruta. Source code in azure_datalake_utils/azure_datalake_utils.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def read_csv_with_partition ( self , ruta : str , partition_cols : Dict [ str , List [ str ]] = None , partition_exclusion : Dict [ str , List [ str ]] = None , partition_inclusion : Dict [ str , List [ str ]] = None , last_modified_last_level : bool = True , ** kwargs : Optional [ Any ], ) -> pd . DataFrame : \"\"\"Leer un archivo CSV desde la cuenta de datalake con particiones Hive. Una partici\u00f3n tipo Hive son archivos almacenados de la forma /ruta/to/archivo/particion_1=1/particion2=2/archivo_con_info.extension. **IMPORTANTE**: por el momento el funcionamiento de esta funci\u00f3n es solo soportada si el objeto fue creado mediante `from_account_key`. Esta funci\u00f3n hace una envoltura de `read_csv` que asu vez usa [pd.read_csv]. usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [pd.read_csv]: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html Args: ruta: Ruta a leeder el archivo, esta ruta debe contener archivos guardados siguiendo la convecci\u00f3n de la partici\u00f3n Hive. Ejemplo si la estructura de los archivos es: - contenedor/ruta/al/archivo/year=2022/month=10/load_date=2022-01-01/archivo.csv - contenedor/ruta/al/archivo/year=2022/month=10/load_date=2022-01-02/archivo.csv - contenedor/ruta/al/archivo/year=2022/month=11/load_date=2022-01-01/archivo.csv `ruta` debe ser `contenedor/ruta/al/archivo/`. partition_cols: Definir que particionews incluir, se asume que se sabe a priori la estructura. Si se pasa `None`, se activa el descubrimiento automatico de las particiones. partition_exclusion: Definir que particiones excluir, es m\u00e1s util cuando no se ha definido `partition_cols=None`. partition_inclusion: Definir la particiones a incluir, es m\u00e1s util cuando no se ha definido `partition_cols=None`. last_modified_last_level: Define si el ultimo nivel no se tiene en cuenta para particiones y se carga el archivo que fue modificado de manera m\u00e1s reciente. **kwargs: argumentos a pasar a pd.read_csv. El unico argumento que es ignorado es storage_options. Returns: Dataframe con la informacion del la ruta. \"\"\" if not ruta . endswith ( \"/\" ): raise ValueError ( \"ruta debe finalizar en /\" ) particiones = HivePartitiion ( ruta = ruta , partition_cols = partition_cols , partition_exclusion = partition_exclusion , partition_inclusion = partition_inclusion , last_modified_last_level = last_modified_last_level , fs = self . fs , ) list_of_files = particiones . get_partition_list () list_of_dfs = [] for path_ , particion in zip ( list_of_files , particiones . get_partition_files ()): particiones_cols = particion [ 1 ] df = self . read_csv ( path_ , ** kwargs ) . assign ( ** particiones_cols ) list_of_dfs . append ( df ) return pd . concat ( list_of_dfs , ignore_index = True ) read_excel ( ruta , experimental = False , ** kwargs ) \u00b6 Leer un archivo Excel desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_excel]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_excel]]:(https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) Parameters: Name Type Description Default ruta str Ruta a leeder el archivo, debe contener una referencia a un archivo .xlsx o .xls . Recordar que la ruta debe contener esta estructura: {NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.xlsx . required force_client Bandera para forzar el uso del cliente. Es flag todavia es experimental Y en futuras versiones se va ha eliminar. Solo funciona con un solo archivo de excel. required **kwargs Optional [ Any ] argumentos a pasar a pd.read_excel. {} Returns: Type Description pd . DataFrame Dataframe con la informacion del la ruta. Source code in azure_datalake_utils/azure_datalake_utils.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 @raiseArchivoNoEncontrado def read_excel ( self , ruta : str , experimental : bool = False , ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo Excel desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_excel]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_excel]]:(https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.xlsx` o `.xls`. Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.xlsx`. force_client: Bandera para forzar el uso del cliente. Es flag todavia es experimental Y en futuras versiones se va ha eliminar. Solo funciona con un solo archivo de excel. **kwargs: argumentos a pasar a pd.read_excel. Returns: Dataframe con la informacion del la ruta. \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) if 'engine' in kwargs : kwargs . pop ( 'engine' ) self . _verificar_extension ( ruta , '.xlsx' , '.xls' ) # TODO: esto es algo temporal y se debe analizar si se puede remover. Esta bandera fue necesario debido a: # 1. Si no se usa el cliente para descargar el excel cuando se modifica el archivo de # excel y no se ha reiniciado # el runtime se provoca el error: `BadZipFile(\"File is not a zip file\")`. # 2. Se debe analizar como integrar en windows donde se usa credenciales asociadas al Active directory. if experimental : df = exp . read_excel_with_client ( ruta , self . datalake_name , self . storage_options [ 'account_key' ], ** kwargs ) return df ############### df = pd . read_excel ( f \"az:// { ruta } \" , engine = 'openpyxl' , storage_options = self . storage_options , ** kwargs ) return df read_json ( ruta , ** kwargs ) \u00b6 Leer un archivo Json desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_json]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_json]]:(https://pandas.pydata.org/docs/reference/api/pandas.read_json.html) Parameters: Name Type Description Default ruta str Ruta a leeder el archivo, debe contener una referencia a un archivo .json . Recordar que la ruta debe contener esta estructura: {NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.json . required **kwargs Optional [ Any ] argumentos a pasar a pd.read_json. {} Returns: Type Description pd . DataFrame Dataframe con la informacion del la ruta. Source code in azure_datalake_utils/azure_datalake_utils.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 @raiseArchivoNoEncontrado def read_json ( self , ruta : str , ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo Json desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_json]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_json]]:(https://pandas.pydata.org/docs/reference/api/pandas.read_json.html) Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.json` . Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.json`. **kwargs: argumentos a pasar a pd.read_json. Returns: Dataframe con la informacion del la ruta. \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) self . _verificar_extension ( ruta , '.json' ) df = pd . read_json ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) return df read_parquet ( ruta , ** kwargs ) \u00b6 Leer un archivo parquet desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_parquet]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_parquet]]:(https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.read_parquet.html) Parameters: Name Type Description Default ruta str Ruta a leeder el archivo, debe contener una referencia a un archivo .json . Recordar que la ruta debe contener esta estructura: {NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.json . required **kwargs Optional [ Any ] argumentos a pasar a pd.read_parquet. {} Returns: Type Description pd . DataFrame dataframe con la ruta Source code in azure_datalake_utils/azure_datalake_utils.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 @raiseArchivoNoEncontrado def read_parquet ( self , ruta : str , ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo parquet desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_parquet]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_parquet]]:(https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.read_parquet.html) Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.json` . Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.json`. **kwargs: argumentos a pasar a pd.read_parquet. Returns: dataframe con la ruta \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) df = pd . read_parquet ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) return df write_csv ( df , ruta , ** kwargs ) \u00b6 Escribir al archivo. Source code in azure_datalake_utils/azure_datalake_utils.py 278 279 280 281 282 283 284 285 def write_csv ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo.\"\"\" if not self . _verificar_extension ( ruta , '.csv' , '.txt' , '.tsv' ): raise ExtensionIncorrecta ( ruta ) sep = kwargs . get ( 'sep' , ',' ) df_to_write = self . _limpiar_df_cols_str ( df , sep ) df_to_write . to_csv ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) write_excel ( df , ruta , ** kwargs ) \u00b6 Escribir al archivo al datalake. Source code in azure_datalake_utils/azure_datalake_utils.py 287 288 289 290 291 def write_excel ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo al datalake.\"\"\" if not self . _verificar_extension ( ruta , '.xlsx' , '.xls' ): raise ExtensionIncorrecta ( ruta ) df . to_excel ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) write_json ( df , ruta , ** kwargs ) \u00b6 Escribir al archivo al datalake. Source code in azure_datalake_utils/azure_datalake_utils.py 293 294 295 296 297 def write_json ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo al datalake.\"\"\" if not self . _verificar_extension ( ruta , '.json' ): raise ExtensionIncorrecta ( ruta ) df . to_json ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) write_parquet ( df , ruta , ** kwargs ) \u00b6 Escribir al archivo al datalake. Source code in azure_datalake_utils/azure_datalake_utils.py 299 300 301 def write_parquet ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo al datalake.\"\"\" df . to_parquet ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs )","title":"Modules"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake","text":"Bases: object Clase para representar operaciones de Datalake. Source code in azure_datalake_utils/azure_datalake_utils.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 class Datalake ( object ): \"\"\"Clase para representar operaciones de Datalake.\"\"\" def __init__ ( self , datalake_name : str , tenant_id : str , account_key : Optional [ str ] = None , fsspec_cache : bool = True ) -> None : \"\"\"Clase para interactuar con Azure Dalake. Args: datalake_name: nombre de la cuenta de Azure Datalake Gen2. tenant_id: Identificador del tenant, es valor es proporcionado por arquitectura de datos, debe conservarse para un correcto funcionamiento. account_key: key de la cuenta. Por defecto es None y es ignorado. fsspec_cache: indica si se va usar la funcionalidad de cache de fsspec. **NOTA**: se recomienda usar `fsspec_cache=False` cuando se encuentran en ambientes serverless, donde no se tiene control escricto de la `vida` de la instancias, de esta manera la libreria siempre verifica la informaci\u00f3n con Azure y no reusar instancias en cache que pueden tener valores desactualizados. Para ver los efectos, ver los siguientes issues: - https://github.com/fsspec/adlfs/issues/391 - https://github.com/Azure/azure-sdk-for-python/issues/28312 \"\"\" self . datalake_name = datalake_name if account_key is None : self . tenant_id = tenant_id credential = exp . AioCredentialWrapper ( InteractiveBrowserCredential ( tenant_id = self . tenant_id )) # TODO: verificar https://github.com/fsspec/adlfs/issues/270 # para ver como evoluciona y evitar este condicional. if platform . system () . lower () != 'windows' : storage_options = { 'account_name' : self . datalake_name , 'anon' : False } else : storage_options = { 'account_name' : self . datalake_name , 'anon' : False , 'credential' : credential , } else : storage_options = { 'account_name' : self . datalake_name , 'account_key' : account_key } self . fs = AzureBlobFileSystem ( account_name = self . datalake_name , account_key = account_key ) if not fsspec_cache : storage_options [ \"default_cache_type\" ] = None storage_options [ \"default_fill_cache\" ] = False storage_options [ \"skip_instance_cache\" ] = True self . storage_options = storage_options @classmethod def from_account_key ( cls , datalake_name : str , account_key : str , fsspec_cache : bool = True ): \"\"\"Opcion de inicializar con account key.\"\"\" return cls ( datalake_name = datalake_name , account_key = account_key , tenant_id = None , fsspec_cache = fsspec_cache ) @raiseArchivoNoEncontrado def read_csv ( self , ruta : Union [ str , List [ str ]], ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo CSV desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_csv]. usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [pd.read_csv]: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.csv` o `.txt`. Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv`. NUEVO en version 0.5: Tambien acepta una lista de archivos terminados en csv. ejemplo: ``` [{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv, {NOMBRE_CONTENEDOR}/{RUTA2}/{nombre o patron}.csv] ``` **kwargs: argumentos a pasar a pd.read_csv. El unico argumento que es ignorado es storage_options. Returns: Dataframe con la informacion del la ruta. \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) if type ( ruta ) == str : self . _verificar_extension ( ruta , '.csv' , '.txt' , '.tsv' ) df = pd . read_csv ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) else : [ self . _verificar_extension ( r , '.csv' , '.txt' , '.tsv' ) for r in ruta ] rutas = [ pd . read_csv ( f \"az:// { r } \" , storage_options = self . storage_options , ** kwargs ) for r in ruta ] df = pd . concat ( rutas , ignore_index = True ) return df @raiseArchivoNoEncontrado def read_excel ( self , ruta : str , experimental : bool = False , ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo Excel desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_excel]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_excel]]:(https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.xlsx` o `.xls`. Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.xlsx`. force_client: Bandera para forzar el uso del cliente. Es flag todavia es experimental Y en futuras versiones se va ha eliminar. Solo funciona con un solo archivo de excel. **kwargs: argumentos a pasar a pd.read_excel. Returns: Dataframe con la informacion del la ruta. \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) if 'engine' in kwargs : kwargs . pop ( 'engine' ) self . _verificar_extension ( ruta , '.xlsx' , '.xls' ) # TODO: esto es algo temporal y se debe analizar si se puede remover. Esta bandera fue necesario debido a: # 1. Si no se usa el cliente para descargar el excel cuando se modifica el archivo de # excel y no se ha reiniciado # el runtime se provoca el error: `BadZipFile(\"File is not a zip file\")`. # 2. Se debe analizar como integrar en windows donde se usa credenciales asociadas al Active directory. if experimental : df = exp . read_excel_with_client ( ruta , self . datalake_name , self . storage_options [ 'account_key' ], ** kwargs ) return df ############### df = pd . read_excel ( f \"az:// { ruta } \" , engine = 'openpyxl' , storage_options = self . storage_options , ** kwargs ) return df @raiseArchivoNoEncontrado def read_json ( self , ruta : str , ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo Json desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_json]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_json]]:(https://pandas.pydata.org/docs/reference/api/pandas.read_json.html) Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.json` . Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.json`. **kwargs: argumentos a pasar a pd.read_json. Returns: Dataframe con la informacion del la ruta. \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) self . _verificar_extension ( ruta , '.json' ) df = pd . read_json ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) return df @raiseArchivoNoEncontrado def read_parquet ( self , ruta : str , ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo parquet desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_parquet]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_parquet]]:(https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.read_parquet.html) Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.json` . Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.json`. **kwargs: argumentos a pasar a pd.read_parquet. Returns: dataframe con la ruta \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) df = pd . read_parquet ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) return df def read_csv_with_partition ( self , ruta : str , partition_cols : Dict [ str , List [ str ]] = None , partition_exclusion : Dict [ str , List [ str ]] = None , partition_inclusion : Dict [ str , List [ str ]] = None , last_modified_last_level : bool = True , ** kwargs : Optional [ Any ], ) -> pd . DataFrame : \"\"\"Leer un archivo CSV desde la cuenta de datalake con particiones Hive. Una partici\u00f3n tipo Hive son archivos almacenados de la forma /ruta/to/archivo/particion_1=1/particion2=2/archivo_con_info.extension. **IMPORTANTE**: por el momento el funcionamiento de esta funci\u00f3n es solo soportada si el objeto fue creado mediante `from_account_key`. Esta funci\u00f3n hace una envoltura de `read_csv` que asu vez usa [pd.read_csv]. usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [pd.read_csv]: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html Args: ruta: Ruta a leeder el archivo, esta ruta debe contener archivos guardados siguiendo la convecci\u00f3n de la partici\u00f3n Hive. Ejemplo si la estructura de los archivos es: - contenedor/ruta/al/archivo/year=2022/month=10/load_date=2022-01-01/archivo.csv - contenedor/ruta/al/archivo/year=2022/month=10/load_date=2022-01-02/archivo.csv - contenedor/ruta/al/archivo/year=2022/month=11/load_date=2022-01-01/archivo.csv `ruta` debe ser `contenedor/ruta/al/archivo/`. partition_cols: Definir que particionews incluir, se asume que se sabe a priori la estructura. Si se pasa `None`, se activa el descubrimiento automatico de las particiones. partition_exclusion: Definir que particiones excluir, es m\u00e1s util cuando no se ha definido `partition_cols=None`. partition_inclusion: Definir la particiones a incluir, es m\u00e1s util cuando no se ha definido `partition_cols=None`. last_modified_last_level: Define si el ultimo nivel no se tiene en cuenta para particiones y se carga el archivo que fue modificado de manera m\u00e1s reciente. **kwargs: argumentos a pasar a pd.read_csv. El unico argumento que es ignorado es storage_options. Returns: Dataframe con la informacion del la ruta. \"\"\" if not ruta . endswith ( \"/\" ): raise ValueError ( \"ruta debe finalizar en /\" ) particiones = HivePartitiion ( ruta = ruta , partition_cols = partition_cols , partition_exclusion = partition_exclusion , partition_inclusion = partition_inclusion , last_modified_last_level = last_modified_last_level , fs = self . fs , ) list_of_files = particiones . get_partition_list () list_of_dfs = [] for path_ , particion in zip ( list_of_files , particiones . get_partition_files ()): particiones_cols = particion [ 1 ] df = self . read_csv ( path_ , ** kwargs ) . assign ( ** particiones_cols ) list_of_dfs . append ( df ) return pd . concat ( list_of_dfs , ignore_index = True ) def write_csv ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo.\"\"\" if not self . _verificar_extension ( ruta , '.csv' , '.txt' , '.tsv' ): raise ExtensionIncorrecta ( ruta ) sep = kwargs . get ( 'sep' , ',' ) df_to_write = self . _limpiar_df_cols_str ( df , sep ) df_to_write . to_csv ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) def write_excel ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo al datalake.\"\"\" if not self . _verificar_extension ( ruta , '.xlsx' , '.xls' ): raise ExtensionIncorrecta ( ruta ) df . to_excel ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) def write_json ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo al datalake.\"\"\" if not self . _verificar_extension ( ruta , '.json' ): raise ExtensionIncorrecta ( ruta ) df . to_json ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) def write_parquet ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo al datalake.\"\"\" df . to_parquet ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) def generar_url_con_sas_token ( self , path : str , duration : int , unit : Literal [ \"day\" , \"hour\" , \"minute\" , 'second' ] = \"hour\" , ip : str = None ) -> str : \"\"\"Genera un link sas.\"\"\" return create_url_sas_token ( path , self . fs , duration , unit , ip ) def _verificar_extension ( self , ruta : str , * extensiones ): \"\"\"Metodo para verificar extensiones.\"\"\" for ext in extensiones : verificar = ruta . endswith ( ext ) if verificar : return True raise ExtensionIncorrecta ( ruta ) def _es_carpeta ( self , ruta : str ): \"\"\"Metodo para verificar si es una carpeta.\"\"\" if ruta . endswith ( \"/\" ): return True raise ExtensionIncorrecta ( f \" { ruta } No termina en /\" ) def _limpiar_df_cols_str ( self , df : pd . DataFrame , sep : str = \",\" ) -> pd . DataFrame : \"\"\"Limpia las columnas string del dataframe.\"\"\" types = df . dtypes string_columns = list ( types [ types == np . array ([ object ()]) . dtype ] . index ) esc_sep = re . escape ( sep ) df_res = df . copy () df_res [ string_columns ] = ( df [ string_columns ] . replace ( esc_sep , \" \" , regex = True ) . replace ( \" \\r \" , \" \" , regex = True ) . replace ( \" \\n \" , \" \" , regex = True ) ) return df_res","title":"Datalake"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake.__init__","text":"Clase para interactuar con Azure Dalake. Parameters: Name Type Description Default datalake_name str nombre de la cuenta de Azure Datalake Gen2. required tenant_id str Identificador del tenant, es valor es proporcionado por arquitectura de datos, debe conservarse para un correcto funcionamiento. required account_key Optional [ str ] key de la cuenta. Por defecto es None y es ignorado. None fsspec_cache bool indica si se va usar la funcionalidad de cache de fsspec. NOTA : se recomienda usar fsspec_cache=False cuando se encuentran en ambientes serverless, donde no se tiene control escricto de la vida de la instancias, de esta manera la libreria siempre verifica la informaci\u00f3n con Azure y no reusar instancias en cache que pueden tener valores desactualizados. Para ver los efectos, ver los siguientes issues: - https://github.com/fsspec/adlfs/issues/391 - https://github.com/Azure/azure-sdk-for-python/issues/28312 True Source code in azure_datalake_utils/azure_datalake_utils.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , datalake_name : str , tenant_id : str , account_key : Optional [ str ] = None , fsspec_cache : bool = True ) -> None : \"\"\"Clase para interactuar con Azure Dalake. Args: datalake_name: nombre de la cuenta de Azure Datalake Gen2. tenant_id: Identificador del tenant, es valor es proporcionado por arquitectura de datos, debe conservarse para un correcto funcionamiento. account_key: key de la cuenta. Por defecto es None y es ignorado. fsspec_cache: indica si se va usar la funcionalidad de cache de fsspec. **NOTA**: se recomienda usar `fsspec_cache=False` cuando se encuentran en ambientes serverless, donde no se tiene control escricto de la `vida` de la instancias, de esta manera la libreria siempre verifica la informaci\u00f3n con Azure y no reusar instancias en cache que pueden tener valores desactualizados. Para ver los efectos, ver los siguientes issues: - https://github.com/fsspec/adlfs/issues/391 - https://github.com/Azure/azure-sdk-for-python/issues/28312 \"\"\" self . datalake_name = datalake_name if account_key is None : self . tenant_id = tenant_id credential = exp . AioCredentialWrapper ( InteractiveBrowserCredential ( tenant_id = self . tenant_id )) # TODO: verificar https://github.com/fsspec/adlfs/issues/270 # para ver como evoluciona y evitar este condicional. if platform . system () . lower () != 'windows' : storage_options = { 'account_name' : self . datalake_name , 'anon' : False } else : storage_options = { 'account_name' : self . datalake_name , 'anon' : False , 'credential' : credential , } else : storage_options = { 'account_name' : self . datalake_name , 'account_key' : account_key } self . fs = AzureBlobFileSystem ( account_name = self . datalake_name , account_key = account_key ) if not fsspec_cache : storage_options [ \"default_cache_type\" ] = None storage_options [ \"default_fill_cache\" ] = False storage_options [ \"skip_instance_cache\" ] = True self . storage_options = storage_options","title":"__init__()"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake.from_account_key","text":"Opcion de inicializar con account key. Source code in azure_datalake_utils/azure_datalake_utils.py 70 71 72 73 @classmethod def from_account_key ( cls , datalake_name : str , account_key : str , fsspec_cache : bool = True ): \"\"\"Opcion de inicializar con account key.\"\"\" return cls ( datalake_name = datalake_name , account_key = account_key , tenant_id = None , fsspec_cache = fsspec_cache )","title":"from_account_key()"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake.generar_url_con_sas_token","text":"Genera un link sas. Source code in azure_datalake_utils/azure_datalake_utils.py 303 304 305 306 307 def generar_url_con_sas_token ( self , path : str , duration : int , unit : Literal [ \"day\" , \"hour\" , \"minute\" , 'second' ] = \"hour\" , ip : str = None ) -> str : \"\"\"Genera un link sas.\"\"\" return create_url_sas_token ( path , self . fs , duration , unit , ip )","title":"generar_url_con_sas_token()"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake.read_csv","text":"Leer un archivo CSV desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de pd.read_csv . usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. Parameters: Name Type Description Default ruta Union [ str , List [ str ]] Ruta a leeder el archivo, debe contener una referencia a un archivo .csv o .txt . Recordar que la ruta debe contener esta estructura: {NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv . NUEVO en version 0.5: Tambien acepta una lista de archivos terminados en csv. ejemplo: [{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv, {NOMBRE_CONTENEDOR}/{RUTA2}/{nombre o patron}.csv] required **kwargs Optional [ Any ] argumentos a pasar a pd.read_csv. El unico argumento que es ignorado es storage_options. {} Returns: Type Description pd . DataFrame Dataframe con la informacion del la ruta. Source code in azure_datalake_utils/azure_datalake_utils.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @raiseArchivoNoEncontrado def read_csv ( self , ruta : Union [ str , List [ str ]], ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo CSV desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_csv]. usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [pd.read_csv]: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.csv` o `.txt`. Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv`. NUEVO en version 0.5: Tambien acepta una lista de archivos terminados en csv. ejemplo: ``` [{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.csv, {NOMBRE_CONTENEDOR}/{RUTA2}/{nombre o patron}.csv] ``` **kwargs: argumentos a pasar a pd.read_csv. El unico argumento que es ignorado es storage_options. Returns: Dataframe con la informacion del la ruta. \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) if type ( ruta ) == str : self . _verificar_extension ( ruta , '.csv' , '.txt' , '.tsv' ) df = pd . read_csv ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) else : [ self . _verificar_extension ( r , '.csv' , '.txt' , '.tsv' ) for r in ruta ] rutas = [ pd . read_csv ( f \"az:// { r } \" , storage_options = self . storage_options , ** kwargs ) for r in ruta ] df = pd . concat ( rutas , ignore_index = True ) return df","title":"read_csv()"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake.read_csv_with_partition","text":"Leer un archivo CSV desde la cuenta de datalake con particiones Hive. Una partici\u00f3n tipo Hive son archivos almacenados de la forma /ruta/to/archivo/particion_1=1/particion2=2/archivo_con_info.extension. IMPORTANTE : por el momento el funcionamiento de esta funci\u00f3n es solo soportada si el objeto fue creado mediante from_account_key . Esta funci\u00f3n hace una envoltura de read_csv que asu vez usa pd.read_csv . usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. Parameters: Name Type Description Default ruta str Ruta a leeder el archivo, esta ruta debe contener archivos guardados siguiendo required la convecci\u00f3n de la partici\u00f3n Hive. Ejemplo si la estructura de los archivos es required partition_cols Dict [ str , List [ str ]] Definir que particionews incluir, se asume que se sabe a priori la estructura. Si se pasa None , se activa el descubrimiento automatico de las particiones. None partition_exclusion Dict [ str , List [ str ]] Definir que particiones excluir, es m\u00e1s util cuando no se ha definido partition_cols=None . None partition_inclusion Dict [ str , List [ str ]] Definir la particiones a incluir, es m\u00e1s util cuando no se ha definido partition_cols=None . None last_modified_last_level bool Define si el ultimo nivel no se tiene en cuenta para particiones y se carga el archivo que fue modificado de manera m\u00e1s reciente. True **kwargs Optional [ Any ] argumentos a pasar a pd.read_csv. El unico argumento que es ignorado es storage_options. {} Returns: Type Description pd . DataFrame Dataframe con la informacion del la ruta. Source code in azure_datalake_utils/azure_datalake_utils.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def read_csv_with_partition ( self , ruta : str , partition_cols : Dict [ str , List [ str ]] = None , partition_exclusion : Dict [ str , List [ str ]] = None , partition_inclusion : Dict [ str , List [ str ]] = None , last_modified_last_level : bool = True , ** kwargs : Optional [ Any ], ) -> pd . DataFrame : \"\"\"Leer un archivo CSV desde la cuenta de datalake con particiones Hive. Una partici\u00f3n tipo Hive son archivos almacenados de la forma /ruta/to/archivo/particion_1=1/particion2=2/archivo_con_info.extension. **IMPORTANTE**: por el momento el funcionamiento de esta funci\u00f3n es solo soportada si el objeto fue creado mediante `from_account_key`. Esta funci\u00f3n hace una envoltura de `read_csv` que asu vez usa [pd.read_csv]. usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [pd.read_csv]: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html Args: ruta: Ruta a leeder el archivo, esta ruta debe contener archivos guardados siguiendo la convecci\u00f3n de la partici\u00f3n Hive. Ejemplo si la estructura de los archivos es: - contenedor/ruta/al/archivo/year=2022/month=10/load_date=2022-01-01/archivo.csv - contenedor/ruta/al/archivo/year=2022/month=10/load_date=2022-01-02/archivo.csv - contenedor/ruta/al/archivo/year=2022/month=11/load_date=2022-01-01/archivo.csv `ruta` debe ser `contenedor/ruta/al/archivo/`. partition_cols: Definir que particionews incluir, se asume que se sabe a priori la estructura. Si se pasa `None`, se activa el descubrimiento automatico de las particiones. partition_exclusion: Definir que particiones excluir, es m\u00e1s util cuando no se ha definido `partition_cols=None`. partition_inclusion: Definir la particiones a incluir, es m\u00e1s util cuando no se ha definido `partition_cols=None`. last_modified_last_level: Define si el ultimo nivel no se tiene en cuenta para particiones y se carga el archivo que fue modificado de manera m\u00e1s reciente. **kwargs: argumentos a pasar a pd.read_csv. El unico argumento que es ignorado es storage_options. Returns: Dataframe con la informacion del la ruta. \"\"\" if not ruta . endswith ( \"/\" ): raise ValueError ( \"ruta debe finalizar en /\" ) particiones = HivePartitiion ( ruta = ruta , partition_cols = partition_cols , partition_exclusion = partition_exclusion , partition_inclusion = partition_inclusion , last_modified_last_level = last_modified_last_level , fs = self . fs , ) list_of_files = particiones . get_partition_list () list_of_dfs = [] for path_ , particion in zip ( list_of_files , particiones . get_partition_files ()): particiones_cols = particion [ 1 ] df = self . read_csv ( path_ , ** kwargs ) . assign ( ** particiones_cols ) list_of_dfs . append ( df ) return pd . concat ( list_of_dfs , ignore_index = True )","title":"read_csv_with_partition()"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake.read_excel","text":"Leer un archivo Excel desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_excel]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_excel]]:(https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) Parameters: Name Type Description Default ruta str Ruta a leeder el archivo, debe contener una referencia a un archivo .xlsx o .xls . Recordar que la ruta debe contener esta estructura: {NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.xlsx . required force_client Bandera para forzar el uso del cliente. Es flag todavia es experimental Y en futuras versiones se va ha eliminar. Solo funciona con un solo archivo de excel. required **kwargs Optional [ Any ] argumentos a pasar a pd.read_excel. {} Returns: Type Description pd . DataFrame Dataframe con la informacion del la ruta. Source code in azure_datalake_utils/azure_datalake_utils.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 @raiseArchivoNoEncontrado def read_excel ( self , ruta : str , experimental : bool = False , ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo Excel desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_excel]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_excel]]:(https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.xlsx` o `.xls`. Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.xlsx`. force_client: Bandera para forzar el uso del cliente. Es flag todavia es experimental Y en futuras versiones se va ha eliminar. Solo funciona con un solo archivo de excel. **kwargs: argumentos a pasar a pd.read_excel. Returns: Dataframe con la informacion del la ruta. \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) if 'engine' in kwargs : kwargs . pop ( 'engine' ) self . _verificar_extension ( ruta , '.xlsx' , '.xls' ) # TODO: esto es algo temporal y se debe analizar si se puede remover. Esta bandera fue necesario debido a: # 1. Si no se usa el cliente para descargar el excel cuando se modifica el archivo de # excel y no se ha reiniciado # el runtime se provoca el error: `BadZipFile(\"File is not a zip file\")`. # 2. Se debe analizar como integrar en windows donde se usa credenciales asociadas al Active directory. if experimental : df = exp . read_excel_with_client ( ruta , self . datalake_name , self . storage_options [ 'account_key' ], ** kwargs ) return df ############### df = pd . read_excel ( f \"az:// { ruta } \" , engine = 'openpyxl' , storage_options = self . storage_options , ** kwargs ) return df","title":"read_excel()"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake.read_json","text":"Leer un archivo Json desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_json]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_json]]:(https://pandas.pydata.org/docs/reference/api/pandas.read_json.html) Parameters: Name Type Description Default ruta str Ruta a leeder el archivo, debe contener una referencia a un archivo .json . Recordar que la ruta debe contener esta estructura: {NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.json . required **kwargs Optional [ Any ] argumentos a pasar a pd.read_json. {} Returns: Type Description pd . DataFrame Dataframe con la informacion del la ruta. Source code in azure_datalake_utils/azure_datalake_utils.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 @raiseArchivoNoEncontrado def read_json ( self , ruta : str , ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo Json desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_json]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_json]]:(https://pandas.pydata.org/docs/reference/api/pandas.read_json.html) Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.json` . Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.json`. **kwargs: argumentos a pasar a pd.read_json. Returns: Dataframe con la informacion del la ruta. \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) self . _verificar_extension ( ruta , '.json' ) df = pd . read_json ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) return df","title":"read_json()"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake.read_parquet","text":"Leer un archivo parquet desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_parquet]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_parquet]]:(https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.read_parquet.html) Parameters: Name Type Description Default ruta str Ruta a leeder el archivo, debe contener una referencia a un archivo .json . Recordar que la ruta debe contener esta estructura: {NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.json . required **kwargs Optional [ Any ] argumentos a pasar a pd.read_parquet. {} Returns: Type Description pd . DataFrame dataframe con la ruta Source code in azure_datalake_utils/azure_datalake_utils.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 @raiseArchivoNoEncontrado def read_parquet ( self , ruta : str , ** kwargs : Optional [ Any ]) -> pd . DataFrame : \"\"\"Leer un archivo parquet desde la cuenta de datalake. Esta funci\u00f3n hace una envoltura de [pd.read_parquet]. Por favor usar la documentaci\u00f3n de la funci\u00f3n para determinar parametros adicionales. [[pd.read_parquet]]:(https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.read_parquet.html) Args: ruta: Ruta a leeder el archivo, debe contener una referencia a un archivo `.json` . Recordar que la ruta debe contener esta estructura: `{NOMBRE_CONTENEDOR}/{RUTA}/{nombre o patron}.json`. **kwargs: argumentos a pasar a pd.read_parquet. Returns: dataframe con la ruta \"\"\" if 'storage_options' in kwargs : kwargs . pop ( 'storage_options' ) df = pd . read_parquet ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs ) return df","title":"read_parquet()"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake.write_csv","text":"Escribir al archivo. Source code in azure_datalake_utils/azure_datalake_utils.py 278 279 280 281 282 283 284 285 def write_csv ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo.\"\"\" if not self . _verificar_extension ( ruta , '.csv' , '.txt' , '.tsv' ): raise ExtensionIncorrecta ( ruta ) sep = kwargs . get ( 'sep' , ',' ) df_to_write = self . _limpiar_df_cols_str ( df , sep ) df_to_write . to_csv ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs )","title":"write_csv()"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake.write_excel","text":"Escribir al archivo al datalake. Source code in azure_datalake_utils/azure_datalake_utils.py 287 288 289 290 291 def write_excel ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo al datalake.\"\"\" if not self . _verificar_extension ( ruta , '.xlsx' , '.xls' ): raise ExtensionIncorrecta ( ruta ) df . to_excel ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs )","title":"write_excel()"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake.write_json","text":"Escribir al archivo al datalake. Source code in azure_datalake_utils/azure_datalake_utils.py 293 294 295 296 297 def write_json ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo al datalake.\"\"\" if not self . _verificar_extension ( ruta , '.json' ): raise ExtensionIncorrecta ( ruta ) df . to_json ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs )","title":"write_json()"},{"location":"api/#azure_datalake_utils.azure_datalake_utils.Datalake.write_parquet","text":"Escribir al archivo al datalake. Source code in azure_datalake_utils/azure_datalake_utils.py 299 300 301 def write_parquet ( self , df : pd . DataFrame , ruta , ** kwargs : Optional [ Any ]) -> None : \"\"\"Escribir al archivo al datalake.\"\"\" df . to_parquet ( f \"az:// { ruta } \" , storage_options = self . storage_options , ** kwargs )","title":"write_parquet()"},{"location":"changelog/","text":"Changelog \u00b6 0.5.10 - 2024-08-26 \u00b6 fixed \u00b6 Tests de read_parquet 0.5.9 - 2024-08-26 \u00b6 Changed \u00b6 Lectura y escritura de archivos parquet , no forzan que sea una carpeta. 0.5.8 - 2023-12-25 \u00b6 Added \u00b6 Lectura y escritura de archivos parquet . Funcionalidad para crear url con sas token Actualizacion de algunas versiones de libreria 0.5.7 - 2023-06-01 \u00b6 Fixed \u00b6 Inclusi\u00f3n de la clase AioCredentialWrapper para realizar implementaci\u00f3n no sincronica de autenticaci\u00f3n interactiva. 0.5.6 - 2023-05-16 \u00b6 Fixed \u00b6 Inclusi\u00f3n de skip_instance_cache para asegurar no usar cache, cuando se incializa con fsspec_cache=False . 0.5.5 - 2023-02-15 \u00b6 Fixed \u00b6 Correci\u00f3n de test unitarios. 0.5.4 - 2023-02-15 \u00b6 Fixed \u00b6 Adicionar storage_options[\"default_fill_cache\"] = False para asegurar no usar el cache, cuando la opci\u00f3n fsspec_cache=False es usada. 0.5.3 - 2023-02-08 \u00b6 Fixed \u00b6 version pip en poetry lock 0.5.1 - 2023-02-08 \u00b6 Added \u00b6 se adiciona el parametro fsspec_cache . Se recomienda usar fsspec_cache=False cuando se encuentran en ambientes serverless. Para mas detalles: https://github.com/fsspec/adlfs/issues/391 https://github.com/Azure/azure-sdk-for-python/issues/28312 0.5.0 - 2022-12-10 \u00b6 Added \u00b6 read_csv_with_partition para inferir y leer archivos con particiones tipo hive . Solo se habilita cuando se usa el account key. read_csv ahora soporta listas de archivos como inputs. 0.4.0 - 2022-12-06 \u00b6 Added \u00b6 flag=True en dl.read_excel Para dar un workaround para la siguiente situaci\u00f3n: Se observa un comportamiento extra\u00f1o bajo estas condiciones: Se importa la libreria en un sesion de python. Se abre el archivo de excel y se reemplaza en el datalake. Usando la misma sesi\u00f3n de python, se trata de leer el archivo con dl.read_excel(\"hacebanalitica-user-servicio/configuracion_valores_salesforce.xlsx\") Una exepci\u00f3n BadZipFile: File is not a zip file es disparada. Este flag es experimental y es una soluci\u00f3n rapida (esperamos que temporal) para entornos donde reiniciar python de manera periodica no es una opci\u00f3n (ejemplo en Azure Functions). Se espera explorar m\u00e1s y incorporar una mejor soluci\u00f3n. 0.3.2 - 2022-09-10 \u00b6 Fixed \u00b6 Bug en la lectura de excel. 0.3.1 - 2022-09-05 \u00b6 Added \u00b6 Limpieza de columnas antes de escribir csv. 0.3.0 - 2022-09-05 \u00b6 Added \u00b6 Soporte python 3.10 Fix para credenciales en Windows Escritura y lectura de Json 0.2.0 - 2022-08-31 \u00b6 Added \u00b6 Primera version util con lectura de excel y csv Adicionar ejemplos de como hacer testing 0.1.1 - 2022-08-30 \u00b6 Added \u00b6 inlcuir primera utilidad 0.0.1 (2022-08-30) \u00b6 First release on PyPI.","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#0510---2024-08-26","text":"","title":"0.5.10 - 2024-08-26"},{"location":"changelog/#fixed","text":"Tests de read_parquet","title":"fixed"},{"location":"changelog/#059---2024-08-26","text":"","title":"0.5.9 - 2024-08-26"},{"location":"changelog/#changed","text":"Lectura y escritura de archivos parquet , no forzan que sea una carpeta.","title":"Changed"},{"location":"changelog/#058---2023-12-25","text":"","title":"0.5.8 - 2023-12-25"},{"location":"changelog/#added","text":"Lectura y escritura de archivos parquet . Funcionalidad para crear url con sas token Actualizacion de algunas versiones de libreria","title":"Added"},{"location":"changelog/#057---2023-06-01","text":"","title":"0.5.7 - 2023-06-01"},{"location":"changelog/#fixed_1","text":"Inclusi\u00f3n de la clase AioCredentialWrapper para realizar implementaci\u00f3n no sincronica de autenticaci\u00f3n interactiva.","title":"Fixed"},{"location":"changelog/#056---2023-05-16","text":"","title":"0.5.6 - 2023-05-16"},{"location":"changelog/#fixed_2","text":"Inclusi\u00f3n de skip_instance_cache para asegurar no usar cache, cuando se incializa con fsspec_cache=False .","title":"Fixed"},{"location":"changelog/#055---2023-02-15","text":"","title":"0.5.5 - 2023-02-15"},{"location":"changelog/#fixed_3","text":"Correci\u00f3n de test unitarios.","title":"Fixed"},{"location":"changelog/#054---2023-02-15","text":"","title":"0.5.4 - 2023-02-15"},{"location":"changelog/#fixed_4","text":"Adicionar storage_options[\"default_fill_cache\"] = False para asegurar no usar el cache, cuando la opci\u00f3n fsspec_cache=False es usada.","title":"Fixed"},{"location":"changelog/#053---2023-02-08","text":"","title":"0.5.3 - 2023-02-08"},{"location":"changelog/#fixed_5","text":"version pip en poetry lock","title":"Fixed"},{"location":"changelog/#051---2023-02-08","text":"","title":"0.5.1 - 2023-02-08"},{"location":"changelog/#added_1","text":"se adiciona el parametro fsspec_cache . Se recomienda usar fsspec_cache=False cuando se encuentran en ambientes serverless. Para mas detalles: https://github.com/fsspec/adlfs/issues/391 https://github.com/Azure/azure-sdk-for-python/issues/28312","title":"Added"},{"location":"changelog/#050---2022-12-10","text":"","title":"0.5.0 - 2022-12-10"},{"location":"changelog/#added_2","text":"read_csv_with_partition para inferir y leer archivos con particiones tipo hive . Solo se habilita cuando se usa el account key. read_csv ahora soporta listas de archivos como inputs.","title":"Added"},{"location":"changelog/#040---2022-12-06","text":"","title":"0.4.0 - 2022-12-06"},{"location":"changelog/#added_3","text":"flag=True en dl.read_excel Para dar un workaround para la siguiente situaci\u00f3n: Se observa un comportamiento extra\u00f1o bajo estas condiciones: Se importa la libreria en un sesion de python. Se abre el archivo de excel y se reemplaza en el datalake. Usando la misma sesi\u00f3n de python, se trata de leer el archivo con dl.read_excel(\"hacebanalitica-user-servicio/configuracion_valores_salesforce.xlsx\") Una exepci\u00f3n BadZipFile: File is not a zip file es disparada. Este flag es experimental y es una soluci\u00f3n rapida (esperamos que temporal) para entornos donde reiniciar python de manera periodica no es una opci\u00f3n (ejemplo en Azure Functions). Se espera explorar m\u00e1s y incorporar una mejor soluci\u00f3n.","title":"Added"},{"location":"changelog/#032---2022-09-10","text":"","title":"0.3.2 - 2022-09-10"},{"location":"changelog/#fixed_6","text":"Bug en la lectura de excel.","title":"Fixed"},{"location":"changelog/#031---2022-09-05","text":"","title":"0.3.1 - 2022-09-05"},{"location":"changelog/#added_4","text":"Limpieza de columnas antes de escribir csv.","title":"Added"},{"location":"changelog/#030---2022-09-05","text":"","title":"0.3.0 - 2022-09-05"},{"location":"changelog/#added_5","text":"Soporte python 3.10 Fix para credenciales en Windows Escritura y lectura de Json","title":"Added"},{"location":"changelog/#020---2022-08-31","text":"","title":"0.2.0 - 2022-08-31"},{"location":"changelog/#added_6","text":"Primera version util con lectura de excel y csv Adicionar ejemplos de como hacer testing","title":"Added"},{"location":"changelog/#011---2022-08-30","text":"","title":"0.1.1 - 2022-08-30"},{"location":"changelog/#added_7","text":"inlcuir primera utilidad","title":"Added"},{"location":"changelog/#001-2022-08-30","text":"First release on PyPI.","title":"0.0.1 (2022-08-30)"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/centraal-api/azure-datalake-utils/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 azure_datalake_utils could always use more documentation, whether as part of the official azure_datalake_utils docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/centraal-api/azure_datalake_utils/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up azure_datalake_utils for local development. Fork the azure_datalake_utils repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/azure-datalake-utils.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.8, 3.9. 1.10 Check https://github.com/centraal-api/azure-datalake-utils/actions and make sure that the tests pass for all supported Python versions. Tips \u00b6 $ poetry run pytest tests/test_azure_datalake_utils.py To run a subset of tests. Deploying \u00b6 A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass. Note for maintaniners \u00b6 In the first versions the dependencies are not stable, for 2 reasons: the versions suggested in the initial template were not fully compatible with the objectives of the library. Some dependencies related to document creation and especially adlfs library and fsspec are not fully understood very well. For the above reasons, in some cases, the release workflow could not work, and to avoid creating patch versions the maintainer may need to \"recycle\" a tag, please follow this gist: git push origin :{tag} git tag -d {tag} git tag {tag} git push origin {tag}","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/centraal-api/azure-datalake-utils/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"azure_datalake_utils could always use more documentation, whether as part of the official azure_datalake_utils docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/centraal-api/azure_datalake_utils/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up azure_datalake_utils for local development. Fork the azure_datalake_utils repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/azure-datalake-utils.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.8, 3.9. 1.10 Check https://github.com/centraal-api/azure-datalake-utils/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#tips","text":"$ poetry run pytest tests/test_azure_datalake_utils.py To run a subset of tests.","title":"Tips"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"contributing/#note-for-maintaniners","text":"In the first versions the dependencies are not stable, for 2 reasons: the versions suggested in the initial template were not fully compatible with the objectives of the library. Some dependencies related to document creation and especially adlfs library and fsspec are not fully understood very well. For the above reasons, in some cases, the release workflow could not work, and to avoid creating patch versions the maintainer may need to \"recycle\" a tag, please follow this gist: git push origin :{tag} git tag -d {tag} git tag {tag} git push origin {tag}","title":"Note for maintaniners"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install azure_datalake_utils, run this command in your terminal: $ pip install azure_datalake_utils This is the preferred method to install azure_datalake_utils, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for azure_datalake_utils can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/centraal-api/azure-datalake-utils Or download the tarball : $ curl -OJL https://github.com/centraal-api/azure-datalake-utils/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install azure_datalake_utils, run this command in your terminal: $ pip install azure_datalake_utils This is the preferred method to install azure_datalake_utils, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for azure_datalake_utils can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/centraal-api/azure-datalake-utils Or download the tarball : $ curl -OJL https://github.com/centraal-api/azure-datalake-utils/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"From source"},{"location":"usage/","text":"Usage \u00b6 To use azure_datalake_utils in a project import azure_datalake_utils","title":"Usage"},{"location":"usage/#usage","text":"To use azure_datalake_utils in a project import azure_datalake_utils","title":"Usage"}]}